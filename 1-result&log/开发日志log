2020/1/29
训练一轮
主要配置 
register(
    'TabularMDPDeterministic-v0',
    entry_point='maml_rl.envs.mdp-deterministic:TabularMDPEnv',
    kwargs={'num_states': 81, 'num_actions': 5},
    max_episode_steps=50
)

# Number of outer-loop updates (ie. number of batches of tasks).
num-batches: 501

# Number of trajectories to sample for each task.
fast-batch-size: 27

训练时间 2小时左右。
all offline training time cost (second) 8386.6758081913
construct MDP time consume (seconds) 580.7706327438354

第二轮训练
construct MDP time consume (seconds) 587.937745809555
501/501 [2:17:55<00:00, 16.52s/it]
all offline training time cost (second) 8275.299672365189



参数设置
max_episode_steps=50 是每个episode的步长
# Number of trajectories to sample for each task.
fast-batch-size: 27
一个任务采27个episode/trajectories


meta-batch-size 5 --num-batches 10


输出数据格式：
avg_train_r 平均训练奖励；
[
[t1,t2,...tn] # 同一个任务采n个episode/trajectories
有 meta-batch-size x --num-batches  个episode
]

avg_valid_r 平均评估奖励；
[
[t1,t2,...tn] # 同一个任务采n个episode/trajectories
有 meta-batch-size x --num-batches  个episode
]



2020/1/30

修改了test-my-plus.py 文件，现在怀疑老的test-my.py无法实现策略在线更新。
参考train.py增加了两条TODO，但似乎还没有效果，下一步计划将policy权值打印出来参考，是否更新了策略。


